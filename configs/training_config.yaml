training:
  # Training hyperparameters
  epochs: 100
  batch_size: 32
  learning_rate: 0.001
  weight_decay: 0.0001
  
  # Optimizer settings
  optimizer: "adam"  # adam, sgd, adamw
  optimizer_params:
    betas: [0.9, 0.999]
    eps: 1e-8
  
  # Learning rate scheduler
  scheduler: "cosine"  # cosine, step, exponential, plateau, none
  scheduler_params:
    T_max: 100
    eta_min: 1e-6
  
  # Loss function
  loss: "auto"  # auto, cross_entropy, mse, bce
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 10
    monitor: "val_loss"
    mode: "min"  # min or max
    min_delta: 0.0001
  
  # Model checkpointing
  checkpoint:
    enabled: true
    save_best: true
    save_last: true
    monitor: "val_loss"
    mode: "min"
    path: "artifacts/models/"
    verbose: true
  
  # Gradient clipping
  gradient_clip:
    enabled: false
    max_norm: 1.0
    norm_type: 2
  
  # Mixed precision training
  mixed_precision:
    enabled: false
    
  # Validation
  validation:
    interval: 1  # Validate every N epochs
    metric: "accuracy"  # accuracy, f1, precision, recall
  
  # Logging
  logging:
    interval: 10  # Log every N batches
    log_gradients: false
    log_weights: false
  
  # Seed for reproducibility
  seed: 42
  
  # Device
  device: "auto"  # auto, cpu, cuda, cuda:0, etc.
